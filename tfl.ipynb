{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary of notebook\n",
    "\n",
    "This notebook is made up of the following sections\n",
    "1. Data Ingest\n",
    "2. Data Wrangling\n",
    "3. Data Exploration \n",
    "4. Modelling \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load relevant libaries \n",
    "import pandas            as pd\n",
    "import pandasql          as psql\n",
    "import requests\n",
    "from bs4                 import BeautifulSoup\n",
    "import io\n",
    "import matplotlib.pyplot as plt\n",
    "import missingno         as msno"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Ingest \n",
    "\n",
    "Ingesting this data presents a unique challenge because it is dynamically uploaded via JavaScript, making standard web scraping methods ineffective. Consequently, three specialised functions have been wrote to handle the ingestion. \n",
    "\n",
    "Note: The scale of the data is 31 million rows and as such loading times can be extensive. Functions have docstrings; please consult them for additional information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_csv_urls(url):\n",
    "    \"\"\"\n",
    "    Function to extract CSV URLs from a webpage.\n",
    "\n",
    "    Args:\n",
    "    url (str): The URL of the webpage to scrape.\n",
    "\n",
    "    Returns:\n",
    "    list: A list of URLs pointing to CSV files found on the webpage.\n",
    "\n",
    "    \"\"\"\n",
    "    csv_urls = []\n",
    "    soup = BeautifulSoup(requests.get(url).text, 'html.parser')\n",
    "    for c in soup.select('contents key'):\n",
    "        if c.text.startswith('usage-stats') and c.text.endswith('.csv'):\n",
    "            if '2021' in c.text or '2022' in c.text or '2023' in c.text:\n",
    "                csv_urls.append('https://cycling.data.tfl.gov.uk/' + c.text)\n",
    "    return csv_urls\n",
    "\n",
    "def read_csv_as_dataframe(csv_url):\n",
    "    \"\"\"\n",
    "    Function to read a CSV file from a URL and return it as a DataFrame.\n",
    "\n",
    "    Args:\n",
    "    csv_url (str): The URL of the CSV file to read.\n",
    "\n",
    "    Returns:\n",
    "    pandas.DataFrame or None: A DataFrame containing the CSV data if successful, otherwise None.\n",
    "\n",
    "    \"\"\"\n",
    "    response = requests.get(csv_url)\n",
    "    # Check if the request was successful\n",
    "    if response.status_code == 200:\n",
    "        # Read CSV content into a DataFrame\n",
    "        df = pd.read_csv(io.BytesIO(response.content))\n",
    "        return df\n",
    "    else:\n",
    "        print(\"Failed to fetch CSV from URL:\", csv_url)\n",
    "        return None\n",
    "\n",
    "def combine_csvs(csv_urls):\n",
    "    \"\"\"\n",
    "    Function to combine multiple CSVs into a single DataFrame.\n",
    "\n",
    "    Args:\n",
    "    csv_urls (list): A list of URLs pointing to CSV files.\n",
    "\n",
    "    Returns:\n",
    "    pandas.DataFrame: A DataFrame containing the combined data from all CSV files.\n",
    "\n",
    "    \"\"\"\n",
    "    dataframes = []\n",
    "    for url in csv_urls:\n",
    "        data = read_csv_as_dataframe(url)\n",
    "        if data is not None:\n",
    "            dataframes.append(data)\n",
    "    # Concatenate all DataFrames into a single DataFrame\n",
    "    combined_df = pd.concat(dataframes, ignore_index=True)\n",
    "    return combined_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ingest Data \n",
    "webpage_url = 'https://s3-eu-west-1.amazonaws.com/cycling.data.tfl.gov.uk/?list-type=2&max-keys=1500'\n",
    "csv_urls = extract_csv_urls(webpage_url)\n",
    "\n",
    "# If Statement to highlight if the functions have been unsuccessful\n",
    "if csv_urls:\n",
    "    data = combine_csvs(csv_urls)\n",
    "    print(\"Combined DataFrame from all CSV files:\")\n",
    "else:\n",
    "    print(\"No CSV URLs found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Wrangling\n",
    "\n",
    "This section includes \n",
    "1. Viewing the data \n",
    "2. Visualise Nulls \n",
    "3. Identify and fix Null introduction \n",
    "4. Cast data types correctly \n",
    "5. Check all dates are within 2021-2023 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick view shows data appears to have nulls\n",
    "data.head(10)\n",
    "\n",
    "# Count nulls by columns. No clear pattern nulls across multiple columns. \n",
    "for column in data.columns:\n",
    "    null_count = data[column].isnull().sum()\n",
    "    print(f\"Column {column}: {null_count} null values\")\n",
    "    \n",
    "# Find Min/Max Start Date vs Start_Date. \n",
    "\n",
    "# Results show that nulls are the result of changing column names on date 2022-09-19. \n",
    "# Columns can be mapped together to create a full data set.\n",
    "\n",
    "# Some column names share similar naming conventions. A postfix has been added to distinguish them.\n",
    "postfix = '_2019'\n",
    "\n",
    "# Identify columns 10-20 by their positional indices as the columns to be postfixed\n",
    "cols_to_postfix = data.columns[9:20]\n",
    "\n",
    "# Apply the postfix to the selected column names\n",
    "new_columns = [col + postfix if col in cols_to_postfix else col for col in subset_data.columns]\n",
    "\n",
    "# Assign the new column names back to the DataFrame\n",
    "data.columns = new_columns\n",
    "\n",
    "# SQL interface used to create a unified data frame without nulls. \n",
    "# TODO: Need to cast data types. \n",
    "# TODO: Check the station ids is this ok.\n",
    "\n",
    "query = \"\"\"\n",
    "SELECT  \n",
    "    CASE \n",
    "        WHEN \"Rental Id\" IS NULL THEN CAST(\"Number_2019\" AS VARCHAR)\n",
    "        ELSE CAST(\"Rental Id\" AS VARCHAR)\n",
    "    END AS \"ID\",\n",
    "    CASE \n",
    "        WHEN \"Duration\" IS NULL \n",
    "        THEN \"Total duration (ms)_2019\"/1000 \n",
    "        ELSE \"Duration\" \n",
    "    END AS Ride_Duration_Seconds,\n",
    "    CASE WHEN \"Bike ID\" IS NULL \n",
    "        THEN \"Bike number_2019\" \n",
    "        ELSE \"Bike ID\" \n",
    "    END AS Bike_ID, \n",
    "    CASE \n",
    "        WHEN \"Start Date\" IS NULL \n",
    "        THEN \"Start date_2019\" \n",
    "        ELSE \"Start Date\" \n",
    "    END AS Start_Date, \n",
    "    CASE \n",
    "        WHEN \"End Date\" IS NULL \n",
    "        THEN \"End date_2019\" \n",
    "        ELSE \"End Date\"\n",
    "    END AS End_Date,\n",
    "    CASE \n",
    "        WHEN \"StartStation Name\" IS NULL \n",
    "        THEN \"Start station_2019\" \n",
    "        ELSE \"StartStation Name\" \n",
    "    END AS Start_Station_Name, \n",
    "    CASE \n",
    "        WHEN \"EndStation Name\" IS NULL \n",
    "        THEN \"End station_2019\" \n",
    "        ELSE \"EndStation Name\"\n",
    "    END AS End_Station_Name,\n",
    "    CASE \n",
    "        WHEN \"StartStation Id\" IS NULL \n",
    "        THEN \"Start station number_2019\" \n",
    "        ELSE \"StartStation Id\" \n",
    "    END AS Start_Station_ID, \n",
    "    CASE \n",
    "        WHEN \"EndStation Id\" IS NULL \n",
    "        THEN \"End station number_2019\" \n",
    "        ELSE \"EndStation Id\"\n",
    "    END AS End_Station_ID\n",
    "FROM subset_data\n",
    "\"\"\"\n",
    "\n",
    "# Query run\n",
    "complete_data = psql.sqldf(query, locals())\n",
    "\n",
    "# Rerun null count \n",
    "# Count nulls by columns. No clear pattern nulls across multiple columns. \n",
    "for column in complete_data.columns:\n",
    "    null_count = complete_data[column].isnull().sum()\n",
    "    print(f\"Column {column}: {null_count} null values\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wip Wrangling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_data = pd.read_csv('subset_data.csv')\n",
    "subset_data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Postfix for column names\n",
    "postfix = '_2019'\n",
    "\n",
    "# Identify columns 10-12 by their positional indices (9, 10, 11 in 0-based indexing)\n",
    "cols_to_postfix = subset_data.columns[9:20]\n",
    "\n",
    "# Apply the postfix to the selected column names\n",
    "new_columns = [col + postfix if col in cols_to_postfix else col for col in subset_data.columns]\n",
    "\n",
    "# Assign the new column names back to the DataFrame\n",
    "subset_data.columns = new_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sorts out column descrepancies. Need to cast data types. Check the station ids is this ok.\n",
    "\n",
    "query = \"\"\"\n",
    "SELECT  \n",
    "    CASE \n",
    "        WHEN \"Rental Id\" IS NULL THEN CAST(\"Number_2019\" AS VARCHAR)\n",
    "        ELSE CAST(\"Rental Id\" AS VARCHAR)\n",
    "    END AS \"ID\",\n",
    "    CASE \n",
    "        WHEN \"Duration\" IS NULL \n",
    "        THEN \"Total duration (ms)_2019\"/1000 \n",
    "        ELSE \"Duration\" \n",
    "    END AS Ride_Duration_Seconds,\n",
    "    CASE WHEN \"Bike ID\" IS NULL \n",
    "        THEN \"Bike number_2019\" \n",
    "        ELSE \"Bike ID\" \n",
    "    END AS Bike_ID, \n",
    "    CASE \n",
    "        WHEN \"Start Date\" IS NULL \n",
    "        THEN \"Start date_2019\" \n",
    "        ELSE \"Start Date\" \n",
    "    END AS Start_Date, \n",
    "    CASE \n",
    "        WHEN \"End Date\" IS NULL \n",
    "        THEN \"End date_2019\" \n",
    "        ELSE \"End Date\"\n",
    "    END AS End_Date,\n",
    "    CASE \n",
    "        WHEN \"StartStation Name\" IS NULL \n",
    "        THEN \"Start station_2019\" \n",
    "        ELSE \"StartStation Name\" \n",
    "    END AS Start_Station_Name, \n",
    "    CASE \n",
    "        WHEN \"EndStation Name\" IS NULL \n",
    "        THEN \"End station_2019\" \n",
    "        ELSE \"EndStation Name\"\n",
    "    END AS End_Station_Name,\n",
    "    CASE \n",
    "        WHEN \"StartStation Id\" IS NULL \n",
    "        THEN \"Start station number_2019\" \n",
    "        ELSE \"StartStation Id\" \n",
    "    END AS Start_Station_ID, \n",
    "    CASE \n",
    "        WHEN \"EndStation Id\" IS NULL \n",
    "        THEN \"End station number_2019\" \n",
    "        ELSE \"EndStation Id\"\n",
    "    END AS End_Station_ID\n",
    "FROM subset_data\n",
    "\"\"\"\n",
    "\n",
    "# Running the query\n",
    "result = psql.sqldf(query, locals())\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Postfix for column names\n",
    "postfix = '_2019'\n",
    "\n",
    "# Identify columns 10-12 by their positional indices (9, 10, 11 in 0-based indexing)\n",
    "cols_to_postfix = combined_df.columns[9:20]\n",
    "\n",
    "# Apply the postfix to the selected column names\n",
    "new_columns = [col + postfix if col in cols_to_postfix else col for col in combined_df.columns]\n",
    "\n",
    "# Assign the new column names back to the DataFrame\n",
    "combined_df.columns = new_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sorts out column descrepancies. Need to cast data types. Check the station ids is this ok.\n",
    "\n",
    "query = \"\"\"\n",
    "SELECT  \n",
    "    CASE \n",
    "        WHEN \"Rental Id\" IS NULL THEN CAST(\"Number_2019\" AS VARCHAR)\n",
    "        ELSE CAST(\"Rental Id\" AS VARCHAR)\n",
    "    END AS \"ID\",\n",
    "    CASE \n",
    "        WHEN \"Duration\" IS NULL \n",
    "        THEN \"Total duration (ms)_2019\"/1000 \n",
    "        ELSE \"Duration\" \n",
    "    END AS Ride_Duration_Seconds,\n",
    "    CASE WHEN \"Bike ID\" IS NULL \n",
    "        THEN \"Bike number_2019\" \n",
    "        ELSE \"Bike ID\" \n",
    "    END AS Bike_ID, \n",
    "    CASE \n",
    "        WHEN \"Start Date\" IS NULL \n",
    "        THEN \"Start date_2019\" \n",
    "        ELSE \"Start Date\" \n",
    "    END AS Start_Date, \n",
    "    CASE \n",
    "        WHEN \"End Date\" IS NULL \n",
    "        THEN \"End date_2019\" \n",
    "        ELSE \"End Date\"\n",
    "    END AS End_Date,\n",
    "    CASE \n",
    "        WHEN \"StartStation Name\" IS NULL \n",
    "        THEN \"Start station_2019\" \n",
    "        ELSE \"StartStation Name\" \n",
    "    END AS Start_Station_Name, \n",
    "    CASE \n",
    "        WHEN \"EndStation Name\" IS NULL \n",
    "        THEN \"End station_2019\" \n",
    "        ELSE \"EndStation Name\"\n",
    "    END AS End_Station_Name,\n",
    "    CASE \n",
    "        WHEN \"StartStation Id\" IS NULL \n",
    "        THEN \"Start station number_2019\" \n",
    "        ELSE \"StartStation Id\" \n",
    "    END AS Start_Station_ID, \n",
    "    CASE \n",
    "        WHEN \"EndStation Id\" IS NULL \n",
    "        THEN \"End station number_2019\" \n",
    "        ELSE \"EndStation Id\"\n",
    "    END AS End_Station_ID\n",
    "FROM combined_df\n",
    "\"\"\"\n",
    "\n",
    "# Running the query\n",
    "result = psql.sqldf(query, locals())\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Null Visualisation \n",
    "\n",
    "# msno.matrix(subset_data)\n",
    "msno.matrix(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Exploration \n",
    "\n",
    "We now have a fully formed data frame the requirement of the client from this eda is as follows \n",
    "\n",
    "Highlighted 3 main areas \n",
    "1. What kinds of usage they might expect to see \n",
    "2. What profile of customers they might have \n",
    "3. Operational concerns such as reliability and supply chain management.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usage "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SQL query to extract avg ride duration and total rides per year\n",
    "query = \"\"\"\n",
    "SELECT  \n",
    "    CASE \n",
    "        WHEN \"End_Date\" LIKE '%2021%' THEN '2021'\n",
    "        WHEN \"End_Date\" LIKE '%2022%' THEN '2022'\n",
    "        WHEN \"End_Date\" LIKE '%2023%' THEN '2023'\n",
    "        ELSE 'Other'\n",
    "    END AS Year,\n",
    "    AVG(Ride_Duration_Seconds) AS Average_Ride_Duration,\n",
    "    COUNT(*) as Total_Rides\n",
    "FROM result\n",
    "WHERE Year NOT LIKE 'Other'\n",
    "GROUP BY \n",
    "    CASE \n",
    "        WHEN \"End_Date\" LIKE '%2021%' THEN '2021'\n",
    "        WHEN \"End_Date\" LIKE '%2022%' THEN '2022'\n",
    "        WHEN \"End_Date\" LIKE '%2023%' THEN '2023'\n",
    "        ELSE 'Other'\n",
    "    END\n",
    "\"\"\"\n",
    "\n",
    "# Run query and save to df\n",
    "usage_date = psql.sqldf(query, locals())\n",
    "\n",
    "# Calculate the YoY growth for both key metrics\n",
    "usage_date['YoY_Growth_Duration'] = usage_date['Average_Ride_Duration'].pct_change() * 100\n",
    "usage_date['YoY_Growth_Rides'] = usage_date['Total_Rides'].pct_change() * 100\n",
    "\n",
    "# Add 'type' column. Needed for plotting actuals vs predicted.\n",
    "usage_date['type'] = 'actual'\n",
    "\n",
    "# Calculate average YoY growth over all years.\n",
    "average_yoy_growth_duration = usage_date['YoY_Growth_Duration'].mean()\n",
    "average_yoy_growth_rides = usage_date['YoY_Growth_Rides'].mean()\n",
    "\n",
    "# Predict Average_Ride_Duration for 2024 using 2023 metrics * average YoY growth metric\n",
    "predicted_2024_duration = usage_date.loc[usage_date['Year'] == '2023', 'Average_Ride_Duration'].values[0] * (1 + average_yoy_growth_duration / 100)\n",
    "predicted_2024_rides = usage_date.loc[usage_date['Year'] == '2023', 'Total_Rides'].values[0] * (1 + average_yoy_growth_rides / 100)\n",
    "\n",
    "# Create the 2024 Row using previous calculations\n",
    "new_row = {\n",
    "    'Year': '2024',\n",
    "    'Average_Ride_Duration': predicted_2024_duration,\n",
    "    'Total_Rides': predicted_2024_rides,\n",
    "    'type': 'predicted'\n",
    "}\n",
    "\n",
    "# Append 2024 to existing dataframe and add avg yoy growth metrics.\n",
    "usage_data_metrics = pd.concat([usage_date, pd.DataFrame([new_row])], ignore_index=True)\n",
    "usage_data_metrics['avg_yearly_yoy_growth_duration'] = average_yoy_growth_duration\n",
    "usage_data_metrics['average_yoy_growth_rides'] = average_yoy_growth_rides\n",
    "usage_data_metrics.head(4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Usage Visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the figure and axes objects\n",
    "fig, ax1 = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "# Plotting Total Rides on the first y-axis\n",
    "ax1.bar(usage_data_metrics['Year'], usage_data_metrics['Total_Rides'], color='#D85604', alpha=0.7, label='Total Rides')\n",
    "ax1.set_xlabel('Year')\n",
    "ax1.set_ylabel('Total Rides')\n",
    "ax1.tick_params('y')\n",
    "\n",
    "# Creating a second y-axis for Average Ride Duration\n",
    "ax2 = ax1.twinx()\n",
    "ax2.plot(usage_data_metrics['Year'], usage_data_metrics['Average_Ride_Duration'], color='black', marker='o', label='Average Ride Duration')\n",
    "ax2.set_ylabel('Average Ride Duration', color='black')\n",
    "ax2.tick_params('y', colors='black')\n",
    "\n",
    "# Adding title and legend\n",
    "fig.suptitle('Ride Usage Statistics Over Years')\n",
    "ax1.legend(loc='upper left')\n",
    "ax2.legend(loc='upper right')\n",
    "\n",
    "# Display the plot\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Customer Profile\n",
    "\n",
    "Time Profile \n",
    "Destination Profile "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SQL query to extract count of rides at hourly aggregate grouped by whether weekend \n",
    "\n",
    "query = \"\"\"\n",
    "WITH dates AS (\n",
    "    SELECT \n",
    "        SUBSTRING(Start_Date, 1, 10) AS date,\n",
    "        SUBSTR(Start_Date, 12, 2) AS hour,\n",
    "        *\n",
    "    FROM \n",
    "        result\n",
    ") \n",
    "\n",
    "SELECT\n",
    "    *,\n",
    "    CASE \n",
    "        WHEN INSTR(date, '-') > 0 THEN DATE(date) -- Format: yyyy-mm-dd\n",
    "        WHEN INSTR(date, '/') > 0 AND LENGTH(date) = 10 THEN DATE(SUBSTR(date, 7, 4) || '-' || SUBSTR(date, 4, 2) || '-' || SUBSTR(date, 1, 2)) -- Format: mm/dd/yyyy\n",
    "        WHEN INSTR(date, '/') > 0 AND LENGTH(date) = 8 THEN DATE(SUBSTR(date, 5, 4) || '-' || SUBSTR(date, 3, 2) || '-' || SUBSTR(date, 1, 2)) -- Format: dd/mm/yyyy\n",
    "        ELSE NULL \n",
    "    END AS normalised_date\n",
    "FROM dates \n",
    "\"\"\"\n",
    "\n",
    "# Run query and save to df\n",
    "time_data  = psql.sqldf(query, locals())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weekend Flag \n",
    "time_data_cleaned = pd.DataFrame(time_data)\n",
    "time_data_cleaned['normalised_date'] = pd.to_datetime(time_data['normalised_date'])\n",
    "time_data_cleaned['is_weekend'] = time_data_cleaned['normalised_date'].dt.dayofweek // 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_data_metrics = time_data_cleaned.groupby(['hour', 'is_weekend']).size().reset_index(name='count')\n",
    "time_data_metrics['hour'] = pd.to_numeric(time_data_metrics['hour'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to normalize the count within each group\n",
    "def min_max_normalize(group):\n",
    "    return (group - group.min()) / (group.max() - group.min())\n",
    "\n",
    "# Apply the normalization within each group\n",
    "time_data_metrics['count_normalised'] = time_data_metrics.groupby('is_weekend')['count'].transform(min_max_normalize)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Customer Profile Visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define colors\n",
    "color1 = '#D85604'  # Weekend color\n",
    "color2 = '#000000'  # Weekday color\n",
    "\n",
    "# Filter the data\n",
    "weekend_data = time_data_metrics[time_data_metrics['is_weekend'] == True]\n",
    "weekday_data = time_data_metrics[time_data_metrics['is_weekend'] == False]\n",
    "\n",
    "# Plotting\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "# Plot weekend line\n",
    "line_weekend, = ax.plot(weekend_data['hour'], weekend_data['count_normalised'], color=color1, label='Weekend')\n",
    "\n",
    "# Plot weekday line\n",
    "line_weekday, = ax.plot(weekday_data['hour'], weekday_data['count_normalised'], color=color2, label='Weekday')\n",
    "\n",
    "# Adding labels and title\n",
    "ax.set_xlabel('Hour')\n",
    "ax.set_ylabel('Percentage of total rides by hour')\n",
    "ax.set_title('Distribution of rides per hour categorised by weekend vs. weekday')\n",
    "\n",
    "# Set x-ticks to show all hours\n",
    "ax.set_xticks(range(24))\n",
    "ax.set_xticklabels(range(24))\n",
    "\n",
    "# Adding legend\n",
    "ax.legend()\n",
    "\n",
    "# Show plot\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reliability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SQL query to extract count of rides at hourly aggregate grouped by whether weekend \n",
    "\n",
    "reliability_query = \"\"\"\n",
    "WITH dates AS (\n",
    "    SELECT \n",
    "        *,\n",
    "        SUBSTR(Start_Date, 1, 10) AS date\n",
    "    FROM \n",
    "        result\n",
    ") \n",
    "SELECT\n",
    "    Bike_Id AS bike_id,\n",
    "    MIN(date) AS first_use, \n",
    "    MAX(date) AS last_use,\n",
    "    julianday(MAX(date)) - julianday(MIN(date)) AS date_difference\n",
    "FROM dates\n",
    "GROUP BY ID\n",
    "\"\"\"\n",
    "\n",
    "# Run query and save to df\n",
    "reliability_data  = psql.sqldf(reliability_query, locals())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reliability_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Customer Profile \n",
    "# - Geographical e.g where people are coming from where they are going  \n",
    "#       - Most common start point/end point \n",
    "# - Habitual Types of people \n",
    "#        - Most popular ride time by hour etc \n",
    "# DS Use Case: Segment customers based on cycling behaviour this could be used to inform marketing dynamic pricing etc. \n",
    "\n",
    "# Operational Concerns \n",
    "# Reliability             - When does bike ID stop appearing in the data e.g life of bike \n",
    "#                         - Avg Life span of a bike \n",
    "#                         - Location of bikes dying \n",
    "# DS Use Case: \n",
    "\n",
    "# Supply chain management - Hot spots of rides     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Use Cases \n",
    "\n",
    "Idea 1 - Supply Chain Management: \n",
    "- Use heat maps to visualize high-demand areas and times.\n",
    "- Analyze peak usage periods to ensure adequate bike availability.\n",
    "- Predictive analytics to forecast demand and optimize bike placement.\n",
    "\n",
    "Idea 2 - Habitual Types of Riders\n",
    "- Segment riders into categories (e.g., early morning riders, weekend riders, commuters) based on trip start times.\n",
    "- Use clustering algorithms to identify patterns in riding habits.\n",
    "\n",
    "Idea 3 - Reliability\n",
    "- Track the first and last appearance of each bike ID in the data.\n",
    "- Calculate the operational lifespan of bikes.\n",
    "- Identify patterns or reasons for bike retirements (e.g., frequent repairs, high usage)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tfl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
